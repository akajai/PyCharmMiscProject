{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-28T11:58:35.846061Z",
     "start_time": "2026-01-28T11:58:35.839865Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "In context leartning\n",
    "generative AI architectures and models, such as transformers, recurrent neural networks (RNNs), diffusion models, generative adversarial networks (GANs) and variational autoencoders (VAEs).\n",
    "\n",
    "tokenization using various libraries such as nltk, spaCy, BertTokenizer, and XLNetTokenizer.\n",
    "\n",
    "Text generation\n",
    "\n",
    "GAN and diffusion models\n",
    "Wavenet :text\n",
    "Chatbot and virtual assistants\n",
    "Health care\n",
    "Finance to predit stock prices\n",
    "Gaming\n",
    "Recomentation systems\n",
    "\n",
    "------------------------------\n",
    "Architecture and models\n",
    "RNN  :Time series loop based design\n",
    "    Fine tuning existing models to better\n",
    "    loop based\n",
    "Transformer:\n",
    "    Text generation and image generation\n",
    "    Feed Back\n",
    "    Enable parallel processing\n",
    "    Self attention\n",
    "GAN\n",
    "    Generator\n",
    "    Discriminator\n",
    "\n",
    "        Real and Fake data\n",
    "    Two neural nets play a minimax game: the generator tries to fool the discriminator.\n",
    "    GANs give you the fastest, sharpest samples but require careful training.\n",
    "VAE\n",
    " Encoder and decoder\n",
    "    pattern from\n",
    "    probability distribution\n",
    "    Encoder → latent variable → decoder. The encoder learns a distribution over latents; the decoder reconstructs.\n",
    "    VAEs offer a principled latent space and stable learning, at the cost of image sharpness.\n",
    "Diffusion models\n",
    "    remove noise\n",
    "    Statical probabilities\n",
    "        Gradually add Gaussian noise to data; learn the reverse denoising process.\n",
    "        Diffusion models are currently the gold‑standard for high‑quality generation, trading sampling speed for fidelity.\n",
    "\n",
    "Genearative AI for NLP\n",
    "    Evolution\n",
    "        Rule based\n",
    "        ML based\n",
    "        Deep learning\n",
    "        Transformers : Sequential data\n",
    "        Generative AI for NLP\n",
    "            sentimentance analysis\n",
    "            Text summarization\n",
    "        LLM name came due to the size of data set\n",
    "        BERT encoder\n",
    "        BART encoder and decoder\n",
    "\n",
    "Halucination\n",
    "    better data  set\n",
    "    Fine tuning pretrained model with domain specific data\n",
    "\n",
    "Tools for Generative Engineer for NLP\n",
    "Dynamic computation graphs (Autograd)\n",
    "TensorFlow Extended (TFX)such as defining, launching, and monitoring.\n",
    "keras TensorFlow a preferred choice for enterprise-level NLP applications.\n",
    "huggingface\n",
    "    platform that offers an open-source library with pretrained models and tools to streamline the process of training and fine-tuning generative AI models. primary NLP\n",
    "    Some useful libraries in Hugging Face\n",
    "Transformers: This is perhaps the most famous library from Hugging Face. It offers many pretrained models for working with text. Using these models, you can perform tasks such as text generation, summarization, translation, classification, and question-answering. The library is designed for PyTorch and TensorFlow.\n",
    "\n",
    "Datasets: This library is designed to easily access and share large-scale data sets and evaluation metrics for NLP. It includes a wide array of data sets in different languages and for various tasks, making it easier for researchers and developers to benchmark and evaluate their models.\n",
    "\n",
    "Tokenizers: This library is optimized for performance and versatility in tokenization, which is a critical step in preparing data for NLP models. It can handle all pre-tokenization requirements needed for models like BERT and GPT.\n",
    "langchain\n",
    "    open-source framework that helps streamline AI application development using large language models (LLMs), significantly improving LLM accessibility and functionality for diverse applications.\n",
    "Pydantic\n",
    "    Pydantic is a Python library that helps you streamline data handling. You can use it to parse and validate your data. It uses Python-type annotations.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn context leartning\\ngenerative AI architectures and models, such as transformers, recurrent neural networks (RNNs), diffusion models, generative adversarial networks (GANs) and variational autoencoders (VAEs).\\n\\ntokenization using various libraries such as nltk, spaCy, BertTokenizer, and XLNetTokenizer.\\n\\nText generation\\n\\nGAN and diffusion models\\nWavenet :text\\nChatbot and virtual assistants\\nHealth care\\nFinance to predit stock prices\\nGaming\\nRecomentation systems\\n\\n------------------------------\\nArchitecture and models\\nRNN  :Time series loop based design\\n    Fine tuning existing models to better\\n    loop based\\nTransformer:\\n    Text generation and image generation\\n    Feed Back\\n    Enable parallel processing\\n    Self attention\\nGAN\\n    Generator\\n    Discriminator\\n\\n        Real and Fake data\\n    Two neural nets play a minimax game: the generator tries to fool the discriminator.\\n    GANs give you the fastest, sharpest samples but require careful training.\\nVAE\\n Encoder and decoder\\n    pattern from\\n    probability distribution\\n    Encoder → latent variable → decoder. The encoder learns a distribution over latents; the decoder reconstructs.\\n    VAEs offer a principled latent space and stable learning, at the cost of image sharpness.\\nDiffusion models\\n    remove noise\\n    Statical probabilities\\n        Gradually add Gaussian noise to data; learn the reverse denoising process.\\n        Diffusion models are currently the gold‑standard for high‑quality generation, trading sampling speed for fidelity.\\n\\nGenearative AI for NLP\\n    Evolution\\n        Rule based\\n        ML based\\n        Deep learning\\n        Transformers : Sequential data\\n        Generative AI for NLP\\n            sentimentance analysis\\n            Text summarization\\n        LLM name came due to the size of data set\\n        BERT encoder\\n        BART encoder and decoder\\n\\nHalucination\\n    better data  set\\n    Fine tuning pretrained model with domain specific data\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T13:24:10.619190Z",
     "start_time": "2026-01-29T13:24:09.896378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "z = x * y + x**2\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  # dz/dx = y + 2x = 3 + 4 = 7\n",
    "print(y.grad)  # dz/dy = x = 2"
   ],
   "id": "5a913c08c4a0dd11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T13:24:33.271977Z",
     "start_time": "2026-01-29T13:24:33.268872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model(x):\n",
    "    if x.sum() > 0:\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "x = torch.tensor([1.0, -0.5], requires_grad=True)\n",
    "y = model(x)\n",
    "y.sum().backward()"
   ],
   "id": "e7170e1c8f45df94",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
